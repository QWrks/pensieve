---
title: "Parallel Analysis for Q -- It Might Just Make Sense."
author: "Maximilian Held"
date: "30 April 2016"
output: pdf_document
---

## Abstract

Factor retention is a contentious issue in Q methodology.
On the one hand, if you over-retain, you risk interpreting spurious factors, where there is no evidence of *shared* subjectivity.
On the other hand, conventional, Eigenvalue-based criteria are often criticized as inadequate for Q, where neither people-variables *nor* item-cases are randomly selected.

I suggest that, from an epistemologically pragmatic and ontologically economical standpoint, factor extraction in Q can be understood as a technical and ideally neutral process of *data compression* necessary to produce the kind of human-readable summary necessary for any abductive reasoning.
In compression as in Q factor extraction, we want to retain as much detail (= many viewpoints) as possible, but must distinguish between signal and noise (= spurious factors).

Using mass simulations and multilevel bootstrapping of actual Q data, I present three results recommending Horn's Monte-Carlo based Parallel Analysis (PA) as an appropriate criterion for factor retention:

1. Even though adding people-variables to a Q-study with any given number of item-cases increases signal *and* noise, the relationship is non-linear, and *adding people-variables* allows greater factor retention (up to a point).
2. For any given number of item-cases there is a number of people-variables with an *optimal signal-to-noise ratio*.
3. Increasing the number of people-variables, additionally retained factors can be shown to be *substantively related, more refined "sub-species"* of the fewer, coarser factors retained on a random subset of fewer people-variables. 

In summary, PA is an appropriately skeptical and rigorous method for the *one falsifiable assertion* of any Q study: that operant subjectivity, is, in fact, *shared*.
